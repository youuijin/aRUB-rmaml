{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epoch=60000, imgc=3, imgsz=28, k_qry=15, k_spt=1, meta_lr=0.001, n_way=5, task_num=32, update_lr=0.01, update_step=5, update_step_test=10)\n",
      "init\n",
      "=> no checkpoint found at 'mamlfgsmeps2_8.pt'\n",
      "Meta(\n",
      "  (net): Learner(\n",
      "    conv2d:(ch_in:3, ch_out:32, k:3x3, stride:1, padding:0)\n",
      "    relu:(True,)\n",
      "    bn:(32,)\n",
      "    max_pool2d:(k:2, stride:2, padding:0)\n",
      "    conv2d:(ch_in:32, ch_out:32, k:3x3, stride:1, padding:0)\n",
      "    relu:(True,)\n",
      "    bn:(32,)\n",
      "    max_pool2d:(k:2, stride:2, padding:0)\n",
      "    conv2d:(ch_in:32, ch_out:32, k:3x3, stride:1, padding:0)\n",
      "    relu:(True,)\n",
      "    bn:(32,)\n",
      "    max_pool2d:(k:2, stride:1, padding:0)\n",
      "    flatten:()\n",
      "    linear:(in:128, out:5)\n",
      "    \n",
      "    (vars): ParameterList(\n",
      "        (0): Parameter containing: [torch.float32 of size 32x3x3x3 (GPU 0)]\n",
      "        (1): Parameter containing: [torch.float32 of size 32 (GPU 0)]\n",
      "        (2): Parameter containing: [torch.float32 of size 32 (GPU 0)]\n",
      "        (3): Parameter containing: [torch.float32 of size 32 (GPU 0)]\n",
      "        (4): Parameter containing: [torch.float32 of size 32x32x3x3 (GPU 0)]\n",
      "        (5): Parameter containing: [torch.float32 of size 32 (GPU 0)]\n",
      "        (6): Parameter containing: [torch.float32 of size 32 (GPU 0)]\n",
      "        (7): Parameter containing: [torch.float32 of size 32 (GPU 0)]\n",
      "        (8): Parameter containing: [torch.float32 of size 32x32x3x3 (GPU 0)]\n",
      "        (9): Parameter containing: [torch.float32 of size 32 (GPU 0)]\n",
      "        (10): Parameter containing: [torch.float32 of size 32 (GPU 0)]\n",
      "        (11): Parameter containing: [torch.float32 of size 32 (GPU 0)]\n",
      "        (12): Parameter containing: [torch.float32 of size 5x128 (GPU 0)]\n",
      "        (13): Parameter containing: [torch.float32 of size 5 (GPU 0)]\n",
      "    )\n",
      "    (vars_bn): ParameterList(\n",
      "        (0): Parameter containing: [torch.float32 of size 32 (GPU 0)]\n",
      "        (1): Parameter containing: [torch.float32 of size 32 (GPU 0)]\n",
      "        (2): Parameter containing: [torch.float32 of size 32 (GPU 0)]\n",
      "        (3): Parameter containing: [torch.float32 of size 32 (GPU 0)]\n",
      "        (4): Parameter containing: [torch.float32 of size 32 (GPU 0)]\n",
      "        (5): Parameter containing: [torch.float32 of size 32 (GPU 0)]\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total trainable tensors: 20229\n",
      "shuffle DB :train, b:4000, 5-way, 1-shot, 15-query, resize:28\n",
      "shuffle DB :test, b:100, 5-way, 1-shot, 15-query, resize:28\n",
      "step: 0 \ttraining acc: [0.21458333 0.23833333 0.24875    0.24791667 0.25291667 0.25125   ]\n",
      "step: 0 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.00416667]\n",
      "step: 10 \ttraining acc: [0.19083333 0.22       0.2225     0.23083333 0.23333333 0.23875   ]\n",
      "step: 10 \ttraining acc_adv: [0. 0. 0. 0. 0. 0.]\n",
      "step: 20 \ttraining acc: [0.19875    0.23125    0.24458333 0.25541667 0.26583333 0.27041667]\n",
      "step: 20 \ttraining acc_adv: [0.      0.      0.      0.      0.      0.00875]\n",
      "step: 30 \ttraining acc: [0.21083333 0.26333333 0.28583333 0.30208333 0.305      0.30416667]\n",
      "step: 30 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.02]\n",
      "step: 40 \ttraining acc: [0.21208333 0.27       0.28458333 0.29166667 0.30291667 0.30208333]\n",
      "step: 40 \ttraining acc_adv: [0.      0.      0.      0.      0.      0.03625]\n",
      "step: 50 \ttraining acc: [0.20291667 0.26875    0.29333333 0.30708333 0.30833333 0.3075    ]\n",
      "step: 50 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.05083333]\n",
      "step: 60 \ttraining acc: [0.19625    0.29041667 0.31416667 0.31708333 0.31625    0.31583333]\n",
      "step: 60 \ttraining acc_adv: [0.   0.   0.   0.   0.   0.07]\n",
      "step: 70 \ttraining acc: [0.20708333 0.25791667 0.28708333 0.30083333 0.30875    0.31708333]\n",
      "step: 70 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.06708333]\n",
      "step: 80 \ttraining acc: [0.20208333 0.25125    0.27916667 0.2825     0.28833333 0.29375   ]\n",
      "step: 80 \ttraining acc_adv: [0.      0.      0.      0.      0.      0.07375]\n",
      "step: 90 \ttraining acc: [0.195      0.27208333 0.29875    0.30583333 0.31166667 0.31166667]\n",
      "step: 90 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.07333333]\n",
      "step: 100 \ttraining acc: [0.19541667 0.27833333 0.30625    0.31291667 0.31333333 0.32166667]\n",
      "step: 100 \ttraining acc_adv: [0.    0.    0.    0.    0.    0.095]\n",
      "step: 110 \ttraining acc: [0.1925     0.28041667 0.31083333 0.32833333 0.32958333 0.3325    ]\n",
      "step: 110 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.08791667]\n",
      "step: 120 \ttraining acc: [0.20916667 0.25833333 0.27125    0.27583333 0.285      0.28916667]\n",
      "step: 120 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.07666667]\n",
      "Test acc: [0.2    0.2358 0.2546 0.2595 0.259  0.2598 0.2603 0.261  0.2612 0.2612\n",
      " 0.2617]\n",
      "Test acc_adv: [0.1892 0.2269 0.2458 0.2512 0.2517 0.254  0.2568 0.2546 0.2556 0.2563\n",
      " 0.2563]\n",
      "Test acc_adv_prior: [0.946  0.9614 0.9673 0.968  0.972  0.977  0.9883 0.975  0.979  0.979\n",
      " 0.9795]\n",
      "step: 0 \ttraining acc: [0.21166667 0.28958333 0.30208333 0.3125     0.31791667 0.31958333]\n",
      "step: 0 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.10458333]\n",
      "step: 10 \ttraining acc: [0.21833333 0.28041667 0.30416667 0.31208333 0.30625    0.30875   ]\n",
      "step: 10 \ttraining acc_adv: [0.         0.         0.         0.         0.         0.08666667]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8048\\959684338.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8048\\959684338.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_spt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_spt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_qry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_qry\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# 0~124 -> batch 32일 때\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m             \u001b[0mtot_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtot_step\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_num\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             '''\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\코드\\R-MAML\\Research\\aRUB+RMAML\\MiniImagenet.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflatten_support_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[0msupport_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflatten_query_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\코드\\R-MAML\\Research\\aRUB+RMAML\\MiniImagenet.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#                                                  transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;31m#                                                  ])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             self.transform = transforms.Compose([lambda x: Image.open(x).convert('RGB'),\n\u001b[0m\u001b[0;32m     65\u001b[0m                                                  \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                                                  \u001b[1;31m# transforms.RandomHorizontalFlip(),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    919\u001b[0m         \"\"\"\n\u001b[0;32m    920\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"transparency\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\PIL\\ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m                             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m                             \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m                                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import  torch, os\n",
    "import  numpy as np\n",
    "from    MiniImagenet import MiniImagenet\n",
    "import  scipy.stats\n",
    "from    torch.utils.data import DataLoader\n",
    "from    torch.optim import lr_scheduler\n",
    "import  random, sys, pickle\n",
    "import  argparse\n",
    "import time\n",
    "from metapgd import Meta\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('../run-final/pgd2/PGD-0.001-0.0003', comment='PGD-0.001-0.0003')\n",
    "\n",
    "def mean_confidence_interval(accs, confidence=0.95):\n",
    "    n = accs.shape[0] \n",
    "    m, se = np.mean(accs), scipy.stats.sem(accs)\n",
    "    h = se * scipy.stats.t._ppf((1 + confidence) / 2, n - 1)\n",
    "    return m, h\n",
    "\n",
    "def main():\n",
    "    \n",
    "    torch.manual_seed(222)\n",
    "    torch.cuda.manual_seed_all(222)\n",
    "    np.random.seed(222)\n",
    "\n",
    "    print(args)\n",
    "    '''\n",
    "    config = [\n",
    "        ('conv2d', [32, 3, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 1, 0]),\n",
    "        ('flatten', []),\n",
    "        ('linear', [args.n_way, 32 * 5 * 5])\n",
    "    ]\n",
    "    '''\n",
    "    config = [\n",
    "        ('conv2d', [32, 3, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 1, 0]),\n",
    "        ('flatten', []),\n",
    "        ('linear', [args.n_way, 32 * 2 * 2])\n",
    "    ]\n",
    "    \n",
    "    device = torch.device('cuda:0')\n",
    "    maml = Meta(args, config, device).to(device)\n",
    "    \n",
    "    \n",
    "    start_epoch = 0\n",
    "    start_step = 0\n",
    "    #filename = 'mamlfgsmeps4_2.pt'\n",
    "    filename = 'mamlfgsmeps2_8.pt'\n",
    "    #maml = Meta(args, config).to(device)\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        start_step = checkpoint['step']\n",
    "        maml.net.load_state_dict(checkpoint['state_dict'])\n",
    "        #maml = maml.to(device)\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(filename, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "    \n",
    "\n",
    "    tmp = filter(lambda x: x.requires_grad, maml.parameters())\n",
    "    num = sum(map(lambda x: np.prod(x.shape), tmp))\n",
    "    print(maml)\n",
    "    print('Total trainable tensors:', num)\n",
    "\n",
    "    # batchsz here means total episode number\n",
    "    mini = MiniImagenet('../../dataset/', mode='train', n_way=args.n_way, k_shot=args.k_spt,\n",
    "                        k_query=args.k_qry,\n",
    "                        batchsz=4000, resize=args.imgsz)\n",
    "    mini_test = MiniImagenet('../../dataset/', mode='test', n_way=args.n_way, k_shot=args.k_spt,\n",
    "                             k_query=args.k_qry,\n",
    "                             batchsz=100, resize=args.imgsz)\n",
    "    tot_step = -args.task_num\n",
    "    \n",
    "    ExecTime = 0 # training 시간\n",
    "    SampleTime = 0 # training 중 adv sample 생성 시간\n",
    "    for epoch in range(30):\n",
    "        # fetch meta_batchsz num of episode each time\n",
    "        t = time.perf_counter()\n",
    "        db = DataLoader(mini, args.task_num, shuffle=True, num_workers=0, pin_memory=True)\n",
    "        for step, (x_spt, y_spt, x_qry, y_qry) in enumerate(db): # 0~124 -> batch 32일 때\n",
    "            tot_step = tot_step + args.task_num\n",
    "            '''\n",
    "            if step == 1:\n",
    "                t = time.perf_counter()\n",
    "            if step == 499:\n",
    "                ExecTime = time.perf_counter() - t\n",
    "                print(ExecTime)\n",
    "            if step == 501:\n",
    "                t = time.perf_counter()\n",
    "            if step == 999:\n",
    "                ExecTime = time.perf_counter() - t\n",
    "                print(ExecTime)\n",
    "            '''\n",
    "            x_spt, y_spt, x_qry, y_qry = x_spt.to(device), y_spt.to(device), x_qry.to(device), y_qry.to(device)\n",
    "\n",
    "            accs, accs_adv, loss_q, loss_q_adv, make_time = maml(x_spt, y_spt, x_qry, y_qry)\n",
    "            SampleTime += make_time\n",
    "            if step % 10 == 0:\n",
    "                print('step:', step, '\\ttraining acc:', accs)\n",
    "                print('step:', step, '\\ttraining acc_adv:', accs_adv)\n",
    "                writer.add_scalar(\"acc/train\", accs[-1],tot_step)\n",
    "                writer.add_scalar(\"acc_adv/train\", accs_adv[-1],tot_step)\n",
    "                writer.add_scalar(\"loss/train\", loss_q,tot_step)\n",
    "                writer.add_scalar(\"loss_adv/train\", loss_q_adv,tot_step)\n",
    "                state = {'epoch': epoch, 'step': step, 'state_dict': maml.net.state_dict()}\n",
    "                torch.save(state, 'mamlfgsmeps4_2.pt')\n",
    "            \n",
    "            if step == 124:  # evaluation -> 학습에는 전혀 영향을 주지 않음, copy network를 사용하므로\n",
    "                ExecTime += time.perf_counter() - t\n",
    "                db_test = DataLoader(mini_test, 1, shuffle=True, num_workers=0, pin_memory=True)\n",
    "                accs_all_test = []\n",
    "                accsadv_all_test = []\n",
    "                accsadvpr_all_test = []\n",
    "                loss_all_test = []\n",
    "                loss_adv_all_test = []\n",
    "                \n",
    "                for x_spt, y_spt, x_qry, y_qry in db_test:\n",
    "                    x_spt, y_spt, x_qry, y_qry = x_spt.squeeze(0).to(device), y_spt.squeeze(0).to(device), \\\n",
    "                                                 x_qry.squeeze(0).to(device), y_qry.squeeze(0).to(device)\n",
    "\n",
    "                    accs, accs_adv, accs_adv_prior, loss_q, loss_q_adv = maml.finetunning(x_spt, y_spt, x_qry, y_qry)\n",
    "                    accs_all_test.append(accs)\n",
    "                    accsadv_all_test.append(accs_adv)\n",
    "                    accsadvpr_all_test.append(accs_adv_prior)\n",
    "                    loss_all_test.append(loss_q.item())\n",
    "                    loss_adv_all_test.append(loss_q_adv.item())\n",
    "                    \n",
    "                # [b, update_step+1]\n",
    "                accs = np.array(accs_all_test).mean(axis=0).astype(np.float16)\n",
    "                accs_adv = np.array(accsadv_all_test).mean(axis=0).astype(np.float16)\n",
    "                accs_adv_prior = np.array(accsadvpr_all_test).mean(axis=0).astype(np.float16)\n",
    "                loss_q = np.array(loss_all_test).mean()\n",
    "                loss_q_adv = np.array(loss_adv_all_test).mean()\n",
    "                print('Test acc:', accs)\n",
    "                print('Test acc_adv:', accs_adv)\n",
    "                print('Test acc_adv_prior:', accs_adv_prior)\n",
    "                #writer.add_scalar(\"acc/test\", accs[-1],tot_step)\n",
    "                #writer.add_scalar(\"acc_adv/test\", accs_adv[-1],tot_step)\n",
    "                #writer.add_scalar(\"acc_adv_prior/test\", accs_adv_prior[-1],tot_step)\n",
    "                #writer.add_scalar(\"loss/test\", loss_q,tot_step)\n",
    "                #writer.add_scalar(\"loss_adv/test\", loss_q_adv,tot_step)\n",
    "                \n",
    "                writer.add_scalar(\"acc/test_epoch\", accs[-1],epoch)\n",
    "                writer.add_scalar(\"acc_adv/test_epoch\", accs_adv[-1],epoch)\n",
    "                writer.add_scalar(\"loss/epoch\", loss_q, epoch)\n",
    "                writer.add_scalar(\"loss_adv/epoch\", loss_q_adv, epoch)\n",
    "                \n",
    "                writer.add_scalar(\"train_time/epoch\", ExecTime, epoch)\n",
    "                writer.add_scalar(\"make_time/epoch\", SampleTime, epoch)\n",
    "            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    argparser = argparse.ArgumentParser()\n",
    "    argparser.add_argument('--epoch', type=int, help='epoch number', default=60000)\n",
    "    argparser.add_argument('--n_way', type=int, help='n way', default=5)\n",
    "    argparser.add_argument('--k_spt', type=int, help='k shot for support set', default=1)\n",
    "    argparser.add_argument('--k_qry', type=int, help='k shot for query set', default=15)\n",
    "    argparser.add_argument('--imgsz', type=int, help='imgsz', default=28)\n",
    "    argparser.add_argument('--imgc', type=int, help='imgc', default=3)\n",
    "    argparser.add_argument('--task_num', type=int, help='meta batch size, namely task num', default=32)\n",
    "    argparser.add_argument('--meta_lr', type=float, help='meta-level outer learning rate', default=0.001) #0.001 - 0.0002 기존\n",
    "    argparser.add_argument('--adv_lr', type=float, help='adv-level learning rate', default=0.0003)\n",
    "    argparser.add_argument('--rho', type=float, help='aRUB-rho', default=0.03)\n",
    "    argparser.add_argument('--update_lr', type=float, help='task-level inner update learning rate', default=0.01)\n",
    "    argparser.add_argument('--update_step', type=int, help='task-level inner update steps', default=5)\n",
    "    argparser.add_argument('--update_step_test', type=int, help='update steps for finetunning', default=10)\n",
    "    \n",
    "    #argparser.add_argument('--fast', action=\"store_true\", help='whether to use fgsm')\n",
    "\n",
    "    args = argparser.parse_args(args=[])\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch, os\n",
    "import  numpy as np\n",
    "from    MiniImagenet import MiniImagenet\n",
    "import  scipy.stats\n",
    "from    torch.utils.data import DataLoader\n",
    "from    torch.optim import lr_scheduler\n",
    "import  random, sys, pickle\n",
    "import  argparse\n",
    "import time\n",
    "from metaaRUB import Meta\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('../run-final/pgd2/aRUB-0.001-0.001-0.03', comment='aRUB-0.001-0.001-0.03')\n",
    "\n",
    "def mean_confidence_interval(accs, confidence=0.95):\n",
    "    n = accs.shape[0] \n",
    "    m, se = np.mean(accs), scipy.stats.sem(accs)\n",
    "    h = se * scipy.stats.t._ppf((1 + confidence) / 2, n - 1)\n",
    "    return m, h\n",
    "\n",
    "def main():\n",
    "    \n",
    "    torch.manual_seed(222)\n",
    "    torch.cuda.manual_seed_all(222)\n",
    "    np.random.seed(222)\n",
    "\n",
    "    print(args)\n",
    "    '''\n",
    "    config = [\n",
    "        ('conv2d', [32, 3, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 1, 0]),\n",
    "        ('flatten', []),\n",
    "        ('linear', [args.n_way, 32 * 5 * 5])\n",
    "    ]\n",
    "    '''\n",
    "    config = [\n",
    "        ('conv2d', [32, 3, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 1, 0]),\n",
    "        ('flatten', []),\n",
    "        ('linear', [args.n_way, 32 * 2 * 2])\n",
    "    ]\n",
    "    \n",
    "    device = torch.device('cuda:0')\n",
    "    maml = Meta(args, config, device).to(device)\n",
    "    \n",
    "    \n",
    "    start_epoch = 0\n",
    "    start_step = 0\n",
    "    #filename = 'mamlfgsmeps4_2.pt'\n",
    "    filename = 'mamlfgsmeps2_8.pt'\n",
    "    #maml = Meta(args, config).to(device)\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        start_step = checkpoint['step']\n",
    "        maml.net.load_state_dict(checkpoint['state_dict'])\n",
    "        #maml = maml.to(device)\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(filename, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "    \n",
    "\n",
    "    tmp = filter(lambda x: x.requires_grad, maml.parameters())\n",
    "    num = sum(map(lambda x: np.prod(x.shape), tmp))\n",
    "    print(maml)\n",
    "    print('Total trainable tensors:', num)\n",
    "\n",
    "    # batchsz here means total episode number\n",
    "    mini = MiniImagenet('../../dataset/', mode='train', n_way=args.n_way, k_shot=args.k_spt,\n",
    "                        k_query=args.k_qry,\n",
    "                        batchsz=4000, resize=args.imgsz)\n",
    "    mini_test = MiniImagenet('../../dataset/', mode='test', n_way=args.n_way, k_shot=args.k_spt,\n",
    "                             k_query=args.k_qry,\n",
    "                             batchsz=100, resize=args.imgsz)\n",
    "    tot_step = -args.task_num\n",
    "    \n",
    "    ExecTime = 0 # training 시간\n",
    "    SampleTime = 0 # training 중 adv sample 생성 시간\n",
    "    for epoch in range(30):\n",
    "        # fetch meta_batchsz num of episode each time\n",
    "        t = time.perf_counter()\n",
    "        db = DataLoader(mini, args.task_num, shuffle=True, num_workers=0, pin_memory=True)\n",
    "        for step, (x_spt, y_spt, x_qry, y_qry) in enumerate(db): # 0~124 -> batch 32일 때\n",
    "            tot_step = tot_step + args.task_num\n",
    "            '''\n",
    "            if step == 1:\n",
    "                t = time.perf_counter()\n",
    "            if step == 499:\n",
    "                ExecTime = time.perf_counter() - t\n",
    "                print(ExecTime)\n",
    "            if step == 501:\n",
    "                t = time.perf_counter()\n",
    "            if step == 999:\n",
    "                ExecTime = time.perf_counter() - t\n",
    "                print(ExecTime)\n",
    "            '''\n",
    "            x_spt, y_spt, x_qry, y_qry = x_spt.to(device), y_spt.to(device), x_qry.to(device), y_qry.to(device)\n",
    "\n",
    "            accs, accs_adv, loss_q, loss_q_adv, make_time = maml(x_spt, y_spt, x_qry, y_qry)\n",
    "            SampleTime += make_time\n",
    "            if step % 10 == 0:\n",
    "                print('step:', step, '\\ttraining acc:', accs)\n",
    "                print('step:', step, '\\ttraining acc_adv:', accs_adv)\n",
    "                writer.add_scalar(\"acc/train\", accs[-1],tot_step)\n",
    "                writer.add_scalar(\"acc_adv/train\", accs_adv[-1],tot_step)\n",
    "                writer.add_scalar(\"loss/train\", loss_q,tot_step)\n",
    "                writer.add_scalar(\"loss_adv/train\", loss_q_adv,tot_step)\n",
    "                state = {'epoch': epoch, 'step': step, 'state_dict': maml.net.state_dict()}\n",
    "                torch.save(state, 'mamlfgsmeps4_2.pt')\n",
    "            \n",
    "            if step == 124:  # evaluation -> 학습에는 전혀 영향을 주지 않음, copy network를 사용하므로\n",
    "                ExecTime += time.perf_counter() - t\n",
    "                db_test = DataLoader(mini_test, 1, shuffle=True, num_workers=0, pin_memory=True)\n",
    "                accs_all_test = []\n",
    "                accsadv_all_test = []\n",
    "                accsadvpr_all_test = []\n",
    "                loss_all_test = []\n",
    "                loss_adv_all_test = []\n",
    "                \n",
    "                for x_spt, y_spt, x_qry, y_qry in db_test:\n",
    "                    x_spt, y_spt, x_qry, y_qry = x_spt.squeeze(0).to(device), y_spt.squeeze(0).to(device), \\\n",
    "                                                 x_qry.squeeze(0).to(device), y_qry.squeeze(0).to(device)\n",
    "\n",
    "                    accs, accs_adv, accs_adv_prior, loss_q, loss_q_adv = maml.finetunning(x_spt, y_spt, x_qry, y_qry)\n",
    "                    accs_all_test.append(accs)\n",
    "                    accsadv_all_test.append(accs_adv)\n",
    "                    accsadvpr_all_test.append(accs_adv_prior)\n",
    "                    loss_all_test.append(loss_q.item())\n",
    "                    loss_adv_all_test.append(loss_q_adv.item())\n",
    "                    \n",
    "                # [b, update_step+1]\n",
    "                accs = np.array(accs_all_test).mean(axis=0).astype(np.float16)\n",
    "                accs_adv = np.array(accsadv_all_test).mean(axis=0).astype(np.float16)\n",
    "                accs_adv_prior = np.array(accsadvpr_all_test).mean(axis=0).astype(np.float16)\n",
    "                loss_q = np.array(loss_all_test).mean()\n",
    "                loss_q_adv = np.array(loss_adv_all_test).mean()\n",
    "                print('Test acc:', accs)\n",
    "                print('Test acc_adv:', accs_adv)\n",
    "                print('Test acc_adv_prior:', accs_adv_prior)\n",
    "                #writer.add_scalar(\"acc/test\", accs[-1],tot_step)\n",
    "                #writer.add_scalar(\"acc_adv/test\", accs_adv[-1],tot_step)\n",
    "                #writer.add_scalar(\"acc_adv_prior/test\", accs_adv_prior[-1],tot_step)\n",
    "                #writer.add_scalar(\"loss/test\", loss_q,tot_step)\n",
    "                #writer.add_scalar(\"loss_adv/test\", loss_q_adv,tot_step)\n",
    "                \n",
    "                writer.add_scalar(\"acc/test_epoch\", accs[-1],epoch)\n",
    "                writer.add_scalar(\"acc_adv/test_epoch\", accs_adv[-1],epoch)\n",
    "                writer.add_scalar(\"loss/epoch\", loss_q, epoch)\n",
    "                writer.add_scalar(\"loss_adv/epoch\", loss_q_adv, epoch)\n",
    "                \n",
    "                writer.add_scalar(\"train_time/epoch\", ExecTime, epoch)\n",
    "                writer.add_scalar(\"make_time/epoch\", SampleTime, epoch)\n",
    "            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    argparser = argparse.ArgumentParser()\n",
    "    argparser.add_argument('--epoch', type=int, help='epoch number', default=60000)\n",
    "    argparser.add_argument('--n_way', type=int, help='n way', default=5)\n",
    "    argparser.add_argument('--k_spt', type=int, help='k shot for support set', default=1)\n",
    "    argparser.add_argument('--k_qry', type=int, help='k shot for query set', default=15)\n",
    "    argparser.add_argument('--imgsz', type=int, help='imgsz', default=28)\n",
    "    argparser.add_argument('--imgc', type=int, help='imgc', default=3)\n",
    "    argparser.add_argument('--task_num', type=int, help='meta batch size, namely task num', default=32)\n",
    "    argparser.add_argument('--meta_lr', type=float, help='meta-level outer learning rate', default=0.001) #0.001 - 0.0002 기존\n",
    "    argparser.add_argument('--adv_lr', type=float, help='adv-level learning rate', default=0.001)\n",
    "    argparser.add_argument('--rho', type=float, help='aRUB-rho', default=0.03)\n",
    "    argparser.add_argument('--update_lr', type=float, help='task-level inner update learning rate', default=0.01)\n",
    "    argparser.add_argument('--update_step', type=int, help='task-level inner update steps', default=5)\n",
    "    argparser.add_argument('--update_step_test', type=int, help='update steps for finetunning', default=10)\n",
    "    \n",
    "    #argparser.add_argument('--fast', action=\"store_true\", help='whether to use fgsm')\n",
    "\n",
    "    args = argparser.parse_args(args=[])\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb4569285eef3a3450cb62085a5b1e0da4bce0af555edc33dcf29baf3acc1368"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
